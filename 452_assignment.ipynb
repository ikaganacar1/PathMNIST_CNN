{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nX5-GXt1XQRB"
   },
   "source": [
    "# **İsmail Kağan Acar** - SE452 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkzNlg_NRgV7"
   },
   "source": [
    "# **Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Development and Training processes initially made from google colab but moved to personal computer some commented code snippets are about that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2458,
     "status": "ok",
     "timestamp": 1742488936524,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "z09f-rinapp6",
    "outputId": "d9a21ad5-a4f2-49ff-efbf-1d8106708771"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3113,
     "status": "ok",
     "timestamp": 1742488939625,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "2912MdFWet5j",
    "outputId": "28883d4f-441a-465c-90a4-1f2ca95a02d0"
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!ls\n",
    "#!cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
    "%pip install medmnist torchsummary\n",
    "#!python -m medmnist save --flag=pathmnist --folder=/content/drive/MyDrive/Colab\\ Notebooks/pathmnist/ --postfix=png --download=True --size=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1742488939649,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "R6Ro7d_YR34C",
    "outputId": "afd18559-1fd9-4873-9346-b744fa97d5bd"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ2KPzQCSj7z"
   },
   "source": [
    "# **MedMNIST (PathMNIST) Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbKT4fRSaP3t"
   },
   "source": [
    "Offical Explanation of Dataset: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "285-3ReYXI9F"
   },
   "source": [
    "#### **Batch Size**\n",
    "Batch size should specified before datasets loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742488939656,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "V7cduk5gXGM6"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrzeLQH5XymU"
   },
   "source": [
    "#### **Dataset Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1742488939680,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "TqId_ZalSMer"
   },
   "outputs": [],
   "source": [
    "data_flag = 'pathmnist'\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rclJGPXiYgVW"
   },
   "source": [
    "#### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742488939686,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "qpzzw-xbYoPe"
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    #transforms.Resize((28, 28)), images that are already preprocessed to 28x28 images\n",
    "\n",
    "    # Augmentation\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomRotation(degrees=(-20, 20)),\n",
    "    #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "\n",
    "    # Normalization\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH8pfyGzZ_3l"
   },
   "source": [
    "```Compose([,])```: Allows to chain the transformations.\n",
    "\n",
    "```ToTensor()```: Turns images to PyTorch tensor.\n",
    "\n",
    "```Normalize()```: normalized_pixel = (original_pixel - mean) / std\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Augmentation:**\n",
    "\n",
    "*Dataset is pretty large so augmentation may decrease the computational cost dramatically.*\n",
    "\n",
    "```RandomHorizontalFlip(),```: Flips image vertically but it may not be benefitial. (Experiment it)\n",
    "\n",
    "```RandomRotation()```: Randomly rotates the image respect to given parameters.\n",
    "\n",
    "```ColorJitter()```: Randomly adjusts brightness, contrast, saturation, and hue.\n",
    "\n",
    "**Data Augmentation affects very bad this model it is not used**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dQmt_l3YV3N"
   },
   "source": [
    "#### **Load the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(random split effects validation in bad way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3787,
     "status": "ok",
     "timestamp": 1742488943489,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "62ZtvnvNTJc1",
    "outputId": "5e080654-7e92-437e-db01-155c4957655b"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "#this part works on colab not on local.\n",
    "\"\"\"\n",
    "!ls /content/drive/MyDrive/Colab\\ Notebooks/pathmnist\n",
    "train_dataset = DataClass(root='./drive/MyDrive/Colab Notebooks/pathmnist/',split='train', transform=data_transform, download=True)\n",
    "test_dataset = DataClass(root='./drive/MyDrive/Colab Notebooks/pathmnist/',split='test', transform=data_transform, download=True)\n",
    "val_dataset = DataClass(root='./drive/MyDrive/Colab Notebooks/pathmnist/',split='val', transform=data_transform, download=True)\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=True)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=True)\n",
    "val_dataset = DataClass(split='val', transform=data_transform, download=True)\n",
    "\n",
    "# Using Random Split affected bad so it is deprecarted at early stages.\n",
    "#train_size = int(len(train_dataset)*0.8)\n",
    "#val_size = int(len(train_dataset) - train_size)\n",
    "#train_dataset, val_dataset = data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                               # Data will be divided to parts (batches).\n",
    "                               # Batch size refers how many sample will be in each batch.\n",
    "                               batch_size=BATCH_SIZE,\n",
    "\n",
    "                               # It will shuffle the data before each epoch to prevent overfitting\n",
    "                               shuffle=True,\n",
    "                               num_workers=5,\n",
    "                               pin_memory=True) # Suppose to utilize more gpu\n",
    "\n",
    "val_loader = data.DataLoader(dataset=val_dataset,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=5,\n",
    "                            shuffle=False)\n",
    " \n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=5,\n",
    "                              shuffle=False)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "print(\"Train size: \",train_size,\" Val Size: \", val_size,\" Test Size: \", test_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RFwRDgzK062"
   },
   "source": [
    "# **Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Config\n",
    "**This part is deactivated after experiments ended and decided for best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1742488943520,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "KSDvK0o-KPos"
   },
   "outputs": [],
   "source": [
    "base_augmentation_conf = {\n",
    "    \"randomrotation\":False,\n",
    "    \"colorjitter\": False,\n",
    "    \"randomhorizontalflip\":False\n",
    "}\n",
    "\n",
    "base_conv_layer_conf = {\n",
    "    \"conv_layer_count\": 2,\n",
    "    \"filters\": 128,\n",
    "    \"kernel_size\": 3,\n",
    "    \"padding\": 'same',\n",
    "    \"pooling\": {\n",
    "        \"kernel_size\": 3,\n",
    "        \"stride\": 2\n",
    "    },\n",
    "    \"dropout\": 0.5,\n",
    "    \"activation\": 'relu',\n",
    "}\n",
    "\n",
    "base_classifier_conf = {\n",
    "    \"neurons\": 512,\n",
    "    \"activation\": 'relu',\n",
    "    \"dropout\":0.5\n",
    "}\n",
    "\n",
    "base_general_conf = {\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "\n",
    "    \"optimizer\": 'adam',  \n",
    "    \"loss_function\":\"CrossEntropyLoss\",\n",
    "}\n",
    "\n",
    "base_optimizer_conf = {\n",
    "    \"learning_rate\": 0.0001, # x10 if sdg\n",
    "    \"momentum_for_sgd\": 0.9,\n",
    "    \"weight_decay_for_sgd\": 0.0001, \n",
    "}\n",
    "\n",
    "base_dataset_info = {\n",
    "    \"train_size\": train_size,\n",
    "    \"val_size\": val_size,\n",
    "    \"test_size\": len(test_dataset),\n",
    "}\n",
    "\n",
    "base_early_stop_conf = {\n",
    "    \"early_stop\": False,\n",
    "    \"early_stop_patience\": 5,\n",
    "    \"early_stop_min_delta\":0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model Hyperparameters\n",
    "These are the hyperparameters that decided after detailed experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_conf = {\n",
    "    \"randomrotation\":False,\n",
    "    \"colorjitter\": False,\n",
    "    \"randomhorizontalflip\":False\n",
    "}\n",
    "\n",
    "conv_layer_conf = {\n",
    "    \"conv_layer_count\": 2,\n",
    "    \"filters\": 128,\n",
    "    \"kernel_size\": 5,\n",
    "    \"padding\": 'same',\n",
    "    \"pooling\": {\n",
    "        \"kernel_size\": 4,\n",
    "        \"stride\": 2\n",
    "    },\n",
    "    \"dropout\": 0.2,\n",
    "    \"activation\": 'relu',\n",
    "}\n",
    "\n",
    "classifier_conf = {\n",
    "    \"neurons\": 2048,\n",
    "    \"activation\": 'relu',\n",
    "    \"dropout\":0.2\n",
    "}\n",
    "\n",
    "general_conf = {\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"optimizer\": 'adam',  \n",
    "    \"loss_function\":\"CrossEntropyLoss\",\n",
    "}\n",
    "\n",
    "optimizer_conf = {\n",
    "    \"learning_rate\": 0.0001, # x10 if sdg\n",
    "}\n",
    "\n",
    "dataset_info = {\n",
    "    \"train_size\": train_size,\n",
    "    \"val_size\": val_size,\n",
    "    \"test_size\": len(test_dataset),\n",
    "}\n",
    "\n",
    "early_stop_conf = {\n",
    "    \"early_stop\": False,\n",
    "    \"early_stop_patience\": 5,\n",
    "    \"early_stop_min_delta\":0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742488943543,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "3ReeKl_MR_6u"
   },
   "outputs": [],
   "source": [
    "def optimizer_function(model):\n",
    "    optimizer = general_conf['optimizer']\n",
    "    learning_rate =  optimizer_conf['learning_rate']\n",
    "  \n",
    "    if optimizer == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    elif optimizer == 'sgd':\n",
    "        return optim.SGD(model.parameters(), \n",
    "                        lr=optimizer_conf['learning_rate'],\n",
    "                        momentum=optimizer_conf['momentum_for_sgd'],\n",
    "                        weight_decay=optimizer_conf['weight_decay_for_sgd'],\n",
    "                        nesterov=True)\n",
    "\n",
    "    elif optimizer == 'rmsprop':\n",
    "        return optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, \n",
    "                               weight_decay=0, momentum=0.9, centered=False)\n",
    "    \n",
    "\n",
    "def loss_function():\n",
    "  loss_function = general_conf['loss_function']\n",
    "  if loss_function == 'CrossEntropyLoss':\n",
    "      return nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ENpnMWchoIo"
   },
   "source": [
    "# **Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742488943545,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "An2vW6XChto9"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        \n",
    "\n",
    "        # Feature Extraction\n",
    "        self.features = nn.Sequential( # For adding components to the model end to end\n",
    "            # Layer 1\n",
    "            nn.Conv2d(3, # input channel, 3 channels RGB\n",
    "                      conv_layer_conf['filters'], # output channel\n",
    "                      kernel_size=conv_layer_conf['kernel_size'],\n",
    "                      padding=conv_layer_conf['padding']), # 'same' preserves input size\n",
    "\n",
    "            nn.BatchNorm2d(conv_layer_conf['filters']),\n",
    "            self.get_activation(conv_layer_conf['activation']), # activation function\n",
    "            nn.MaxPool2d(conv_layer_conf['pooling']['kernel_size'], stride=conv_layer_conf['pooling']['stride']), # Downsampling, reduces size\n",
    "            nn.Dropout(conv_layer_conf['dropout']), # deletes random neurons\n",
    "            \n",
    "            # adds how many layers needed to be added (* operator used for unpack list so list elements can be used as arguments of Sequential)\n",
    "            # First layer is special because it takes 3 as first argument os it is added manually\n",
    "            # (if it is commented out, because after 3 layers max pooling becomes an issue. I will make experiments 4 5 6 conv layers manual)\n",
    "            #*[self.additional_conv_layer() for _ in range(conv_layer_conf[\"conv_layer_count\"]-1)],\n",
    "            \n",
    "            self.additional_conv_layer_without_pooling(), #2\n",
    "            self.additional_conv_layer(), #3\n",
    "            self.additional_conv_layer(), #4 # remove pooling if you add 5th layer\n",
    "            #self.additional_conv_layer(), #5\n",
    "            #self.additional_conv_layer_without_pooling(),#6\n",
    "        )\n",
    "\n",
    "        self._to_linear = None\n",
    "        self.calculate_maxpool()\n",
    "\n",
    "        # Classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), # Converts multidimensional array to one dimension\n",
    "            \n",
    "            nn.Linear(self._to_linear, classifier_conf['neurons']),#FC1 \n",
    "            nn.BatchNorm1d(classifier_conf['neurons']),\n",
    "            self.get_activation(classifier_conf['activation']), # activation function\n",
    "            nn.Dropout(classifier_conf['dropout']),\n",
    "            \n",
    "            nn.Linear(classifier_conf['neurons'], 9),# FC2\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def calculate_maxpool(self):\n",
    "        with torch.no_grad(): \n",
    "            dummy_input = torch.randn(1, 3, 28, 28)  # (batch_size, channels, height, width)\n",
    "            dummy_output = self.features(dummy_input)\n",
    "            \n",
    "            self._to_linear = dummy_output.view(1, -1).size(1) # the size after all opearations\n",
    "   \n",
    "    def additional_conv_layer(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(conv_layer_conf['filters'],\n",
    "                        conv_layer_conf['filters'],\n",
    "                        kernel_size=conv_layer_conf['kernel_size'],\n",
    "                        padding=conv_layer_conf['padding']),\n",
    "\n",
    "            nn.BatchNorm2d(conv_layer_conf['filters']),\n",
    "            self.get_activation(conv_layer_conf['activation']), # activation function\n",
    "            nn.MaxPool2d(kernel_size=conv_layer_conf['pooling']['kernel_size'], stride=conv_layer_conf['pooling']['stride']),\n",
    "            nn.Dropout(conv_layer_conf['dropout']),\n",
    "        )\n",
    "\n",
    "    def additional_conv_layer_without_pooling(self):\n",
    "        # For the 4th and further conv layer counts;\n",
    "        # MaxPooling creates an issue because dimension drops significantly\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(conv_layer_conf['filters'],\n",
    "                        conv_layer_conf['filters'],\n",
    "                        kernel_size=conv_layer_conf['kernel_size'],\n",
    "                        padding=conv_layer_conf['padding']),\n",
    "\n",
    "            nn.BatchNorm2d(conv_layer_conf['filters']),\n",
    "            self.get_activation(conv_layer_conf['activation']), # activation function\n",
    "            # !!! #nn.MaxPool2d(kernel_size=conv_layer_conf['pooling']['kernel_size'], stride=conv_layer_conf['pooling']['stride']),\n",
    "            nn.Dropout(conv_layer_conf['dropout']),\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_activation(self, activation_name): # Helper Function for Automation\n",
    "        activations = {\n",
    "            'relu': nn.ReLU,\n",
    "            'sigmoid': nn.Sigmoid,\n",
    "            'tanh': nn.Tanh,\n",
    "        }\n",
    "        return activations[activation_name]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out model structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dummy_model = Model().to(device)\n",
    "summary(dummy_model,(3,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv5XdRmbYWrM"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Early Stop**\n",
    "This algorithm stops training when validation loss stopped improving.\n",
    "probably it is unneccesary here in this model because of low epoch counts.\n",
    "\n",
    "**Early stop implemented but never used after experimention guideline published**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "early_stopper = EarlyStopper(patience=early_stop_conf['early_stop_patience'], min_delta=early_stop_conf['early_stop_min_delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Start Training**\n",
    "\n",
    "Calling this function starts training for active HyperParameter config.\n",
    "It returns val. and train losses and accuracies along with the used model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1098344,
     "status": "ok",
     "timestamp": 1742490041923,
     "user": {
      "displayName": "İsmail Kağan Acar (ikagan.acar)",
      "userId": "03699932260073596399"
     },
     "user_tz": -180
    },
    "id": "ifmzVa5MYV_a",
    "outputId": "a93db88e-26d4-4e96-da02-1db3c386e045"
   },
   "outputs": [],
   "source": [
    "def start_train():\n",
    "    # Connect to GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "\n",
    "    # Model initialization.\n",
    "    model = Model().to(device)\n",
    "    #summary(model,(3,28,28))\n",
    "    \n",
    "    # Loss Function\n",
    "    criterion = loss_function()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optimizer_function(model)\n",
    "\n",
    "    # Epoch counter\n",
    "    number_of_epochs = general_conf['epochs']\n",
    "    \n",
    "    \n",
    "    train_loss_list, val_loss_list = [], []\n",
    "    train_accuracy_list, val_accuracy_list = [], []\n",
    "\n",
    "    for epoch in range(number_of_epochs):\n",
    "        # Train Mode activated\n",
    "        model.train()\n",
    "\n",
    "        # Initializing counters\n",
    "        train_running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                # Input shape comes in [128,1] but optimizer only accept 1D\n",
    "                # Squeeze function makes it 1D\n",
    "                targets = targets.squeeze()\n",
    "\n",
    "                # Actual learing happens here\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # metric calculation for each sample\n",
    "                train_running_loss += loss.item()\n",
    "\n",
    "                # predict with max probability becomes the predict\n",
    "                _, predicted = outputs.max(1)\n",
    "                train_total += targets.size(0)\n",
    "                train_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Loss and accuracy\n",
    "        train_loss = train_running_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "        # Switches to Evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initializing counters\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():# prevents updating gradients\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.squeeze()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_accuracy_list.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{number_of_epochs}]\",\n",
    "            f\"Train Loss: {train_loss_list[-1]:.4f}, Train Acc: {train_accuracy_list[-1]:.4f} |\",\n",
    "            f\"Val Loss: {val_loss_list[-1]:.4f}, Val Acc: {val_accuracy_list[-1]:.4f}\")\n",
    "        \n",
    "        if early_stop_conf['early_stop'] and early_stopper.early_stop(val_loss):    \n",
    "            print(\"Early Stopped Training.\")\n",
    "            break\n",
    "\n",
    "    return train_loss_list,val_loss_list,train_accuracy_list,val_accuracy_list, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used for automation. It logs training metrics and related data to the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def train_and_log(log_name):\n",
    "    try:    \n",
    "        print(log_name, \"Started\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_loss, val_loss, train_acc, val_acc, model = start_train() # start training\n",
    "        end_time = time.time()\n",
    "\n",
    "        duration = end_time - start_time\n",
    "        time_took = f\"{duration // 60} min {duration % 60} sec\"\n",
    "        print(time_took, \"Ended\")\n",
    "\n",
    "        log_entry = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"time_took\":time_took,\n",
    "            \"metrics\": {\n",
    "                \"train_loss\": [float(i) for i in train_loss],\n",
    "                \"val_loss\": [float(i) for i in val_loss],\n",
    "                \"train_accuracy\": [float(i) for i in train_acc],\n",
    "                \"val_accuracy\": [float(i) for i in val_acc]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        log_file = f\"experiments/{log_name}.json\"\n",
    "        \n",
    "        with open(log_file, \"w+\") as file:\n",
    "            json.dump(log_entry, file, indent=2)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(log_name, \"- Failed\")\n",
    "        with open(\"errors.log\",\"a+\") as file:\n",
    "            file.write(str(e))\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Customized Grid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is for making experiments automatically. Its biggest limitation is changing optimizer and batch sizes.The parameters will be used individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [ #training will take forever :(\n",
    "    {\"kernel_size\":[1,2,3,5,7,9]},\n",
    "    {\"conv_dropout\":[0.0,0.1,0.2,0.3,0.4,0.5]},\n",
    "    {\"conv_layer_count\":[1,2,3,4,5,6]},\n",
    "    {\"neurons\":[32,64,128,256,512,1024]},\n",
    "    {\"pool_kernel_size\":[1,2,3,4,5,6]},\n",
    "    {\"pool_stride\":[1,2,3,4,5,6]},\n",
    "    {\"classifier_dropout\":[0.0,0.1,0.2,0.3,0.4,0.5]},\n",
    "    {\"epochs\":[10,15,20,25,30,50]},\n",
    "    {'activation_function':['relu','tanh','sigmoid']}\n",
    "]\n",
    "\n",
    "special_cases = [\n",
    "    \"batch_size\",\n",
    "    \"optimizer_type\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "# i discovered that without this library and deepcopy(),\n",
    "# copying dict to another varible creates a pointer so,\n",
    "# my back up dict's updates when original dict updates.abs\n",
    "# I wasted training 6 hours of 13 models without knowing that :'( \n",
    "\n",
    "\n",
    "conv_conf_backup = copy.deepcopy(conv_layer_conf)\n",
    "classifier_conf_backup = copy.deepcopy(classifier_conf)\n",
    "general_conf_backup = copy.deepcopy(general_conf)\n",
    "\n",
    "# Automated Experiments \n",
    "experiment_check = False\n",
    "if experiment_check == True:\n",
    "        for parameter in experiment_list:\n",
    "                key = list(parameter.items())[0][0]\n",
    "                value_list = list(parameter.items())[0][1]\n",
    "                \n",
    "                for value in value_list:\n",
    "\n",
    "                        if key == 'kernel_size':\n",
    "                                conv_layer_conf[\"kernel_size\"] = value\n",
    "\n",
    "                        elif key == 'conv_dropout':\n",
    "                                conv_layer_conf[\"dropout\"] = value\n",
    "\n",
    "                        elif key == 'conv_layer_count':\n",
    "                                conv_layer_conf[\"conv_layer_count\"] = value\n",
    "\n",
    "                        elif key == 'neurons':\n",
    "                                classifier_conf[\"neurons\"] = value\n",
    "\n",
    "                        elif key == 'pool_kernel_size':\n",
    "                                conv_layer_conf[\"pooling\"][\"kernel_size\"] = value\n",
    "\n",
    "                        elif key == 'pool_stride':\n",
    "                                conv_layer_conf[\"pooling\"][\"stride\"] = value\n",
    "\n",
    "                        elif key == 'classifier_dropout':\n",
    "                                classifier_conf[\"dropout\"] = value\n",
    "\n",
    "                        elif key == 'epochs':\n",
    "                                general_conf[\"epochs\"] = value\n",
    "                        \n",
    "                        elif key == 'activation_function':\n",
    "                                conv_layer_conf[\"activation\"] = value\n",
    "                                classifier_conf[\"activation\"] = value\n",
    "\n",
    "                                \n",
    "                        log_name = f\"{key}-{value}\"\n",
    "                        model = train_and_log(log_name=log_name)\n",
    "\n",
    "                        \n",
    "                        conv_layer_conf = copy.deepcopy(conv_conf_backup)\n",
    "                        classifier_conf = copy.deepcopy(classifier_conf_backup)\n",
    "                        general_conf = copy.deepcopy(general_conf_backup)\n",
    "\n",
    "else:\n",
    "        model = train_and_log(\"custom_model_name-0\") # for manual training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N36_f5mD6-MA"
   },
   "source": [
    "# **Evaluation and Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Saved Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import json\n",
    "\n",
    "results_grouped = dict()\n",
    "\n",
    "for index,file in enumerate(listdir(\"experiments\")):\n",
    "    path= f\"experiments/{file}\"\n",
    "    exp_name = str(file).split('.json')[0]\n",
    "    exp_type, exp_value =exp_name.split('-',1)\n",
    "    with open(path,\"r\") as file_data:\n",
    "        exp_result = json.load(file_data)\n",
    "\n",
    "    if index == 0:\n",
    "            previous_exp_type = exp_type\n",
    "            results_grouped[exp_type] = [{exp_value: exp_result}]\n",
    "    else:\n",
    "        if exp_type == previous_exp_type:\n",
    "\n",
    "            results_grouped[exp_type].append({exp_value: exp_result})\n",
    "        else:\n",
    "            previous_exp_type = exp_type\n",
    "            \n",
    "            try:\n",
    "                results_grouped[exp_type].append({exp_value: exp_result})\n",
    "            except:\n",
    "                results_grouped[exp_type] = [{exp_value: exp_result}]\n",
    "\n",
    "#print(json.dumps(results_grouped,indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Accuracy and Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_to_plot = ['train_loss', 'val_loss', 'train_accuracy', 'val_accuracy']  \n",
    "\n",
    "#figure sizes \n",
    "plt.figure(figsize=(5 * 4, 5 * len(results_grouped)))  \n",
    "\n",
    "\n",
    "for row_idx, exp_group in enumerate(results_grouped, 1):\n",
    "    experiments = results_grouped[exp_group]\n",
    "    \n",
    "    for col_idx, metric in enumerate(metrics_to_plot, 1):\n",
    "        plt.subplot(len(results_grouped), 4, (row_idx - 1) * 4 + col_idx)\n",
    "        \n",
    "        for experiment in experiments:\n",
    "            for name in experiment:\n",
    "                if metric in experiment[name]['metrics']:\n",
    "                    plt.plot(\n",
    "                        experiment[name]['metrics'][metric],\n",
    "                        label=f'{name}'\n",
    "                    )\n",
    "         \n",
    "        plt.ylabel(\"value\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.title(f'{exp_group} - {metric}', fontsize=10)\n",
    "        plt.grid(True)\n",
    "        plt.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Validation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I got help from ai for how can i display and create pandas dataframe for better visualization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for exp_group in results_grouped:\n",
    "    table_data = []\n",
    "    for experiment in results_grouped[exp_group]:\n",
    "        for exp_value in experiment:  \n",
    "            \n",
    "            metrics = experiment[exp_value]['metrics']\n",
    "            table_data.append({\n",
    "                'experiments': f\"{exp_group}-{exp_value}\",\n",
    "\n",
    "                'mean_val_loss': sum(metrics['val_loss'])/len(metrics['val_loss']),\n",
    "                'mean_val_acc': sum(metrics['val_accuracy'])/len(metrics['val_accuracy']),\n",
    "                'best_val_loss': min(metrics['val_loss']),\n",
    "                'best_val_acc': max(metrics['val_accuracy']),\n",
    "\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(table_data)\n",
    "\n",
    "    df = df.sort_values(by=['best_val_acc', 'best_val_loss'], ascending=[False, True])\n",
    "\n",
    "    display(df.style\n",
    "        .format({\n",
    "            'mean_val_loss': '{:.4f}',\n",
    "            'mean_val_acc': '{:.2%}',\n",
    "            'best_val_loss': '{:.4f}',\n",
    "            'best_val_acc': '{:.2%}'\n",
    "        })\n",
    "        .background_gradient(subset=['best_val_acc'], cmap='Greens')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Plot Training Durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_data = []\n",
    "total = 0\n",
    "for exp_group in results_grouped:\n",
    "    \n",
    "    for experiment in results_grouped[exp_group]:\n",
    "        for exp_value in experiment:  \n",
    "            string = experiment[exp_value][\"time_took\"]\n",
    "            \n",
    "            minute , rest = string.split(\" min \")\n",
    "            seconds, _ = rest.split(\" sec\")\n",
    "            time_in_seconds= float(minute)*60 + float(seconds)\n",
    "            total += time_in_seconds\n",
    "            time_in_minutes = round(time_in_seconds/60,2)\n",
    "            time_in_minutes = float(time_in_minutes//1)+round(float((time_in_minutes % 1) *60 )/100,2)\n",
    "\n",
    "            metrics = experiment[exp_value]['metrics']\n",
    "            table_data.append({\n",
    "                'experiments': f\"{exp_group}-{exp_value}\",\n",
    "\n",
    "                'time_took (min.sec)': time_in_minutes\n",
    "\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(table_data)\n",
    "\n",
    "df = df.sort_values(by=['time_took (min.sec)'], ascending=[False])\n",
    "\n",
    "print((total/60)/60)\n",
    "\n",
    "display(df.style\n",
    "    .format({\n",
    "        'time_took (min.sec)': '{}',\n",
    "    })\n",
    "    .background_gradient(subset=['time_took (min.sec)'], cmap='Greens')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing from Trained Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_best_model_trained = True \n",
    "if is_best_model_trained:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "\n",
    "    #model = Model().to(device)\n",
    "    #model.load_state_dict(torch.load(\"trained_model.pt\"))\n",
    "    #model = torch.load('trained_model.pt')\n",
    "    criterion = loss_function()\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            targets = targets.squeeze()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = correct / total\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, tick_marks)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "\n",
    "    #Classification report\n",
    "    precision = precision_score(all_targets, all_preds, average='weighted')\n",
    "    recall = recall_score(all_targets, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"\\nClassification Metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_targets, all_preds, target_names=[str(i) for i in range(9)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"model\") # saving model for later uses"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPRAALajV1HctT6gvMD3KtP",
   "gpuType": "T4",
   "mount_file_id": "1AEMBIRUa9awnOSJeiQrdWrTIWWvfmESJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
